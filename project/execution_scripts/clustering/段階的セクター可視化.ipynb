{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作モジュール\n",
    "from utils.paths import Paths\n",
    "from facades.stock_acquisition_facade import StockAcquisitionFacade\n",
    "from utils.jquants_api_utils import cli\n",
    "from calculation.target_calculator import TargetCalculator\n",
    "# 基本モジュール\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# クラスタリングで使用\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from pyclustering.cluster.xmeans import xmeans, splitting_type\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 結果の描画に使用\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 銘柄情報の取得\n",
    "* stock_lists: 2014年10月からの銘柄一覧\n",
    "* history_list: 銘柄ごとのScaleCategoryの遍歴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition = \"(Listing==1)&((ScaleCategory=='TOPIX Core30')|(ScaleCategory=='TOPIX Large70')|(ScaleCategory=='TOPIX Mid400')|(ScaleCategory=='TOPIX Small 1'))\" #現行のTOPIX500\"\"\n",
    "saf = StockAcquisitionFacade(filter=filter_condition) #filtered_code_list = filter_codes)\n",
    "stock_dfs = saf.get_stock_data_dict()\n",
    "stock_dfs['list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時系列データの成型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数（日内リターン）を算出\n",
    "stock_dfs['price']['Target'] = stock_dfs['price']['Close'] / stock_dfs['price']['Open'] - 1\n",
    "target = stock_dfs['price'][['Date', 'Code', 'Target']]\n",
    "target = target.set_index(['Date', 'Code'], drop=True).unstack(-1).droplevel(0, axis=1)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 関数群"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pca_residues(target: pd.DataFrame, remove_components: int, end_date: datetime = datetime(2021, 12, 31)):\n",
    "    '''\n",
    "    remove_componentsで指定した数までの主成分を除去して、残差を返します。\n",
    "    '''\n",
    "    target = target[target.index <= end_date]\n",
    "    no_missing_target = target.dropna(axis=1).T\n",
    "    if remove_components == 0:\n",
    "        return no_missing_target\n",
    "    pca = PCA(n_components = remove_components).fit(no_missing_target)\n",
    "    pca_array = pca.transform(no_missing_target)\n",
    "    inversed_array = pca.inverse_transform(pca_array)\n",
    "\n",
    "    return no_missing_target - inversed_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAPの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_UMAP(df:pd.DataFrame, n_components:int=15, n_neighbors:int=5, min_dist:float=0.1, metric:str='euclidean'):\n",
    "    UMAP_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, metric=metric, random_state=42)\n",
    "    UMAP_result = UMAP_model.fit_transform(df)\n",
    "    reduced_df = pd.DataFrame(UMAP_result, index=df.index, columns=['Feature '+ str(i) for i in range(0, n_components)])\n",
    "    reduced_df = reduced_df.sort_index(ascending=True).reset_index(drop=False)\n",
    "    return reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 社名の追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_company_name(df, config_df):\n",
    "    return pd.merge(config_df[['Code', 'CompanyName']], df, on='Code').set_index(['Code', 'CompanyName'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xmeansによるクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_xmeans(df, kmax: int, amount_centers: str = 2, tolerrance:float = 0.001):\n",
    "    xm_c = kmeans_plusplus_initializer(data=df, amount_centers=2).initialize()\n",
    "    xm_i = xmeans(data=df, initial_centers=xm_c, kmax=kmax, tolerrance=0.001, ccore=True)\n",
    "    xm_i.process()\n",
    "\n",
    "    centers = xm_i.get_centers()\n",
    "    df['Cluster'] = 999\n",
    "    df['ClusterCenter0'] = 0.0\n",
    "    df['ClusterCenter1'] = 0.0\n",
    "    for cluster_num, cluster in enumerate(xm_i.get_clusters()):\n",
    "        df.iloc[cluster, df.columns.get_loc('Cluster')] = cluster_num\n",
    "        df.iloc[cluster, df.columns.get_loc('ClusterCenter0')] = centers[cluster_num][0]\n",
    "        df.iloc[cluster, df.columns.get_loc('ClusterCenter1')] = centers[cluster_num][1]\n",
    "    \n",
    "    df['DistanceFromClusterCenter'] = (\n",
    "        (df['Feature 0'] - df['ClusterCenter0']) ** 2 + \\\n",
    "        (df['Feature 1'] - df['ClusterCenter1']) ** 2\n",
    "        ) ** 0.5\n",
    "\n",
    "\n",
    "        \n",
    "    return df[['Feature 0', 'Feature 1', 'Cluster', 'DistanceFromClusterCenter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_hdbscan(df, min_cluster_size: int = 10, min_samples: int = 5, *args, **kwargs):\n",
    "    hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, *args, **kwargs)\n",
    "    df['Cluster'] = hdbscan.fit_predict(df)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_in_plotly(df, n_clusters=5):\n",
    "    \"\"\"\n",
    "    マルチインデックス（銘柄コードと社名）を持つUMAPデータをクラスタリングして可視化する\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        UMAPの結果を含むデータフレーム（マルチインデックス[Code, CompanyName]付き）\n",
    "    n_clusters : int\n",
    "        クラスター数\n",
    "    \"\"\"\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from sklearn.cluster import KMeans\n",
    "    import pandas as pd\n",
    "    \n",
    "    # インデックスから情報を取得\n",
    "    df_plot = df.copy()\n",
    "    \n",
    "    # マルチインデックスをリセットして通常の列に変換\n",
    "    df_plot = df_plot.reset_index()\n",
    "    \n",
    "    # 散布図の作成\n",
    "    fig = px.scatter(\n",
    "        df_plot,\n",
    "        x='Feature 0',\n",
    "        y='Feature 1',\n",
    "        color='Cluster',\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        hover_data={\n",
    "            'Code': True,\n",
    "            'CompanyName': True,\n",
    "            'Cluster': True,\n",
    "            'Feature 0': ':.4f',\n",
    "            'Feature 1': ':.4f'\n",
    "        },\n",
    "        title='UMAP次元削減による株価データの可視化',\n",
    "        labels={'Feature 0': 'UMAP次元1', 'Feature 1': 'UMAP次元2', 'Cluster': 'クラスター'},\n",
    "        opacity=0.7\n",
    "    )\n",
    " \n",
    "    \n",
    "    # ホバーテンプレートをカスタマイズ（銘柄コード、社名、クラスターを表示）\n",
    "    fig.update_traces(\n",
    "        selector=dict(type='scatter', mode='markers'),\n",
    "        hovertemplate='<b>銘柄コード: %{customdata[0]}</b><br><b>社名: %{customdata[1]}</b><br><b>クラスター: %{customdata[2]}</b><br>UMAP次元1: %{x:.4f}<br>UMAP次元2: %{y:.4f}<extra></extra>'\n",
    "    )\n",
    "    \n",
    "    # レイアウトの調整\n",
    "    fig.update_layout(\n",
    "        width=900,\n",
    "        height=700,\n",
    "        plot_bgcolor='white',\n",
    "        legend_title_text='クラスター',\n",
    "        xaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='lightgray',\n",
    "            zeroline=True,\n",
    "            zerolinewidth=1,\n",
    "            zerolinecolor='lightgray',\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showgrid=True,\n",
    "            gridwidth=1,\n",
    "            gridcolor='lightgray',\n",
    "            zeroline=True,\n",
    "            zerolinewidth=1,\n",
    "            zerolinecolor='lightgray',\n",
    "        ),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 図を表示\n",
    "    fig.show()\n",
    "    \n",
    "    # クラスタリング結果を含むデータフレームを返す\n",
    "    # マルチインデックスに戻す\n",
    "    df_result = df_plot.set_index(['Code', 'CompanyName'])\n",
    "    \n",
    "    return fig, df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes =[\"5310\",\"4047\",\"3101\",\"4045\",\"4403\",\"4021\",\"4272\",\"5901\",\"4028\",\"4044\",\"4095\",\"4046\",\"4212\",\"4617\",\"4008\",\"4023\",\"4041\",\"4996\",\"4634\",\"4216\",\"4078\",\"4633\",\"4401\",\"4109\",\"4471\",\"4114\",\"4088\",\"4091\",\"8012\",\"8098\",\"3401\",\"3405\",\"4203\",\"3402\",\"3407\",\"4205\",\"4631\",\"4043\",\"4042\",\"4182\",\"4208\",\"4183\",\"4005\",\"4188\",\"4061\",\"4118\",\"4202\"]\n",
    "\n",
    "target2 = target.loc[:, codes]\n",
    "target2.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "remove_pc = 0\n",
    "target2 = target\n",
    "target2 = target2[target2.index <= datetime(2021, 12, 31)]\n",
    "no_missing_target = target2.dropna(axis=1)\n",
    "no_missing_target = no_missing_target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_means = []\n",
    "\n",
    "for reduce_components in range(len(no_missing_target) - 1):\n",
    "    residual_df = extract_pca_residues(no_missing_target, reduce_components).T\n",
    "    corr_mean = ((residual_df.corr().apply(abs).sum() - 1) / len(residual_df.columns)).mean()\n",
    "    print(f'{reduce_components}: {corr_mean}')\n",
    "    corr_means.append(corr_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "remove_pc = 0\n",
    "#target2 = target\n",
    "target2 = target2[target2.index <= datetime(2021, 12, 31)]\n",
    "no_missing_target = target2.dropna(axis=1).T\n",
    "\n",
    "#for pca_components in range(2, len(no_missing_target.index)):\n",
    "for pca_components in range(10, 26):\n",
    "    pca = PCA(n_components = pca_components).fit(no_missing_target)\n",
    "    pca_array = pca.transform(no_missing_target)\n",
    "    inversed_array = pca.inverse_transform(pca_array)\n",
    "    print(f'累積寄与率：{sum(pca.explained_variance_ratio_)}')\n",
    "    target_pcaed = pd.DataFrame(inversed_array, index=no_missing_target.index, \n",
    "                        columns=pd.to_datetime(no_missing_target.columns)).T\n",
    "\n",
    "    residual_df = extract_pca_residues(target_pcaed, remove_pc)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors=2, min_dist=0.01, metric='correlation')\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_hdbscan(df, min_cluster_size=2, min_samples=1, cluster_selection_epsilon = 0.7)\n",
    "    df = df[['Cluster']].rename(columns={'Cluster': f'Cluster{pca_components}'})\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=1)\n",
    "\n",
    "def compare_rows(df):\n",
    "    \"\"\"\n",
    "    データフレームの各行を互いに比較し、一致する値の数をカウントする新しいデータフレームを返す\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        入力データフレーム\n",
    "    code_col : str, default='Code'\n",
    "        コード列の名前\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        各行のペア間で一致する値の数を示す新しいデータフレーム\n",
    "    \"\"\"\n",
    "    \n",
    "    # 結果を格納するための空のデータフレームを作成\n",
    "    result = pd.DataFrame(index=df.index, columns=df.index)\n",
    "    \n",
    "    # 各行のペアを比較\n",
    "    for idx1 in df.index:\n",
    "        for idx2 in df.index:\n",
    "            # 同じ値を持つ列の数をカウント\n",
    "            matches = (df.loc[idx1] == df.loc[idx2]).sum()\n",
    "            result.loc[idx1, idx2] = matches\n",
    "    \n",
    "    return result #- np.eye(len(result.index)) * (len(result.index) - 1)\n",
    "\n",
    "\n",
    "result = compare_rows(df)\n",
    "df.to_csv(f'Clusters.csv')\n",
    "result.to_csv(f'CompareResult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = 10\n",
    "remove_pc = 0\n",
    "\n",
    "#target2 = target\n",
    "target2 = target2[target2.index <= datetime(2021, 12, 31)]\n",
    "no_missing_target = target2.dropna(axis=1).T\n",
    "pca = PCA(n_components = pca_components).fit(no_missing_target)\n",
    "pca_array = pca.transform(no_missing_target)\n",
    "inversed_array = pca.inverse_transform(pca_array)\n",
    "print(f'累積寄与率：{sum(pca.explained_variance_ratio_)}')\n",
    "target_pcaed = pd.DataFrame(inversed_array, index=no_missing_target.index, \n",
    "                      columns=pd.to_datetime(no_missing_target.columns)).T\n",
    "\n",
    "residual_df = extract_pca_residues(target_pcaed, remove_pc)\n",
    "umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors=2, min_dist=0.01, metric='correlation')\n",
    "df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "df = apply_hdbscan(df, min_cluster_size=2, min_samples=1, cluster_selection_epsilon = 0.7)\n",
    "print(f'除去する主成分数：{remove_pc}')\n",
    "show_in_plotly(df)\n",
    "df.to_csv(f'ClustersRemove{remove_pc}PCs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セクター判定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove_pc in range(0, 10):\n",
    "    residual_df = extract_pca_residues(target2, remove_pc)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors = 3, min_dist=0, metric='cosine')\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_xmeans(df, kmax=50)\n",
    "    print(f'除去する主成分数：{remove_pc}')\n",
    "    show_in_plotly(df)\n",
    "    df.to_csv(f'ClustersRemove{remove_pc}PCs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for remove_pc in range(0, 10):\n",
    "    residual_df = extract_pca_residues(target2, remove_pc)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors = 3, min_dist=0, metric='cosine')\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_xmeans(df, kmax=50)\n",
    "    print(f'除去する主成分数：{remove_pc}')\n",
    "    show_in_plotly(df)\n",
    "    df.to_csv(f'ClustersRemove{remove_pc}PCs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検討"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サブプロットを作成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def show_umap_subplots(df_dict, n_neighbors_list, min_dist_list):\n",
    "    \"\"\"\n",
    "    複数のUMAPパラメータ設定による結果を4×4のサブプロットとして可視化\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_dict : dict\n",
    "        キー: (n_neighbors, min_dist) のタプル\n",
    "        値: 対応するパラメータでのUMAP結果のデータフレーム\n",
    "    n_neighbors_list : list\n",
    "        n_neighborsの値のリスト [3, 5, 10, 15]\n",
    "    min_dist_list : list\n",
    "        min_distの値のリスト [0, 0.01, 0.05, 0.1]\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        プロットされた図\n",
    "    \"\"\"\n",
    "    # サブプロットタイトル\n",
    "    subplot_titles = [f'n_neighbors={n}, min_dist={d}' \n",
    "                     for n in n_neighbors_list \n",
    "                     for d in min_dist_list]\n",
    "    \n",
    "    # サブプロットの作成\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=4,\n",
    "        subplot_titles=subplot_titles,\n",
    "        vertical_spacing=0.05,\n",
    "        horizontal_spacing=0.05\n",
    "    )\n",
    "    \n",
    "    # カラーシーケンスの取得\n",
    "    color_sequence = px.colors.qualitative.Bold\n",
    "    \n",
    "    # 各パラメータ組み合わせのプロットを追加\n",
    "    for i, n_neighbors in enumerate(n_neighbors_list):\n",
    "        for j, min_dist in enumerate(min_dist_list):\n",
    "            # 該当するパラメータ組み合わせのデータフレームを取得\n",
    "            key = (n_neighbors, min_dist)\n",
    "            if key not in df_dict:\n",
    "                continue\n",
    "                \n",
    "            df_plot = df_dict[key].copy().reset_index()\n",
    "            \n",
    "            # ユニーククラスターを取得\n",
    "            clusters = df_plot['Cluster'].unique()\n",
    "            \n",
    "            # 各クラスターを別々のトレースとして追加\n",
    "            for cluster_id in sorted(clusters):\n",
    "                cluster_df = df_plot[df_plot['Cluster'] == cluster_id]\n",
    "                color_idx = cluster_id % len(color_sequence)\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=cluster_df['Feature 0'],\n",
    "                        y=cluster_df['Feature 1'],\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=6,\n",
    "                            color=color_sequence[color_idx],\n",
    "                            opacity=0.7,\n",
    "                            line=dict(width=0.5, color='white')\n",
    "                        ),\n",
    "                        name=f'Cluster {cluster_id}',\n",
    "                        legendgroup=f'Cluster {cluster_id}',\n",
    "                        showlegend=False,  # サブプロットが多いため凡例は非表示\n",
    "                        customdata=np.column_stack((\n",
    "                            cluster_df['Code'],\n",
    "                            cluster_df['CompanyName'],\n",
    "                            cluster_df['Cluster']\n",
    "                        )),\n",
    "                        hovertemplate=(\n",
    "                            '<b>銘柄コード: %{customdata[0]}</b><br>' +\n",
    "                            '<b>社名: %{customdata[1]}</b><br>' +\n",
    "                            '<b>クラスター: %{customdata[2]}</b><br>' +\n",
    "                            'UMAP次元1: %{x:.4f}<br>' +\n",
    "                            'UMAP次元2: %{y:.4f}<br>' +\n",
    "                            '<extra></extra>'\n",
    "                        )\n",
    "                    ),\n",
    "                    row=i+1, col=j+1\n",
    "                )\n",
    "            \n",
    "            # クラスタ数をアノテーションとして追加\n",
    "            cluster_count = len(clusters)\n",
    "            fig.add_annotation(\n",
    "                x=0.5, y=0.02,\n",
    "                text=f'クラスター数: {cluster_count}',\n",
    "                showarrow=False,\n",
    "                xref=f'x{i*4+j+1}', yref=f'y{i*4+j+1}',\n",
    "                font=dict(size=10, color=\"black\"),\n",
    "                bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "                bordercolor=\"black\",\n",
    "                borderwidth=1,\n",
    "                borderpad=4,\n",
    "                row=i+1, col=j+1\n",
    "            )\n",
    "    \n",
    "    # レイアウト設定\n",
    "    fig.update_layout(\n",
    "        title='UMAP パラメータ比較 (4×4 サブプロット)',\n",
    "        height=900,\n",
    "        width=1200,\n",
    "        template=\"plotly_white\",\n",
    "        margin=dict(l=20, r=20, t=50, b=20),\n",
    "        hoverlabel=dict(\n",
    "            bgcolor=\"white\",\n",
    "            font_size=12,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # 軸のレイアウト設定\n",
    "    fig.update_xaxes(\n",
    "        showticklabels=False,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        zerolinewidth=1,\n",
    "        zerolinecolor='lightgray',\n",
    "        gridwidth=1,\n",
    "        gridcolor='lightgray',\n",
    "        title_text=''\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(\n",
    "        showticklabels=False,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        zerolinewidth=1,\n",
    "        zerolinecolor='lightgray',\n",
    "        gridwidth=1,\n",
    "        gridcolor='lightgray',\n",
    "        title_text=''\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# 使用例\n",
    "\"\"\"\n",
    "# 例えば以下のようにして使用します:\n",
    "n_neighbors_list = [3, 5, 10, 15]\n",
    "min_dist_list = [0, 0.01, 0.05, 0.1]\n",
    "\n",
    "# 結果を格納する辞書\n",
    "df_dict = {}\n",
    "\n",
    "# 各パラメータ組み合わせの処理\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    for min_dist in min_dist_list:\n",
    "        # UMAPとクラスタリングを実行\n",
    "        residual_df = extract_pca_residues(target, 0)\n",
    "        umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors=n_neighbors, min_dist=min_dist, metric='correlation')\n",
    "        df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "        df = apply_xmeans(df, kmax=50)\n",
    "        \n",
    "        # 結果を辞書に保存\n",
    "        df_dict[(n_neighbors, min_dist)] = df\n",
    "\n",
    "# サブプロットの可視化\n",
    "fig = show_umap_subplots(df_dict, n_neighbors_list, min_dist_list)\n",
    "fig.show()\n",
    "\n",
    "# 保存する場合\n",
    "# fig.write_html(\"umap_parameter_comparison.html\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_neighborsとmin_distの検討"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_list = [2, 3, 5, 10]\n",
    "min_dist_list = [0.001, 0.1, 0.3, 0.5]\n",
    "df_dict = {}\n",
    "df_list_for_csv = []\n",
    "\n",
    "\n",
    "target = target[target.index <= datetime(2021, 12, 31)]\n",
    "no_missing_target = target.dropna(axis=1).T\n",
    "pca = PCA(n_components = 150).fit(no_missing_target)\n",
    "pca_array = pca.transform(no_missing_target)\n",
    "inversed_array = pca.inverse_transform(pca_array)\n",
    "print(f'累積寄与率：{sum(pca.explained_variance_ratio_)}')\n",
    "target_pcaed = pd.DataFrame(inversed_array, index=no_missing_target.index, \n",
    "                      columns=pd.to_datetime(no_missing_target.columns)).T\n",
    "\n",
    "for n_neighbors in n_neighbors_list:\n",
    "    for min_dist in min_dist_list:\n",
    "        residual_df = extract_pca_residues(target_pcaed, 0)\n",
    "        umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors = n_neighbors, min_dist=min_dist, metric='correlation')\n",
    "        df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "        df = apply_hdbscan(df, min_cluster_size=2, min_samples=1, cluster_selection_epsilon = 0.3)\n",
    "        df_dict[(n_neighbors, min_dist)] = df\n",
    "        df_list_for_csv.append(df[['Cluster']].rename(columns={'Cluster': f'{n_neighbors}nbs_{min_dist}_md'}))\n",
    "        \n",
    "final_df = pd.concat(df_list_for_csv, axis=1)\n",
    "final_df.to_csv('Clusters_UmapParamExploring.csv')\n",
    "\n",
    "fig = show_umap_subplots(df_dict, n_neighbors_list, min_dist_list)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### umapのパラメータ検討"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_neighbors（小さいほど局所に注目）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [5, 15, 25, 35, 45]:\n",
    "    residual_df = extract_pca_residues(target, 1)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, n_neighbors = n_neighbors, metric='euclidean')\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_xmeans(df, kmax=5)\n",
    "    print(f'n_neighbors: {n_neighbors}')\n",
    "    show_in_plotly(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_dist（小さいほどクラスタの凝集度が高まる）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for min_dist in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n",
    "    residual_df = extract_pca_residues(target, 1)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, min_dist = min_dist, n_neighbors = 5, metric='euclidean')\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_xmeans(df, kmax=5)\n",
    "    print(f'min_dist: {min_dist}')\n",
    "    show_in_plotly(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### metric (\"euclidean\": ユークリッド距離、\"cosine\": ベクトル情報を使用)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 各metricの簡易説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. euclidean（ユークリッド距離）:\n",
    "    - 最も基本的な直線距離。\n",
    "    - 連続値データに対して直感的でよく使われるが、ノイズにやや敏感。\n",
    "\n",
    " 2. manhattan（マンハッタン距離）:\n",
    "    - 各次元の絶対差の合計（碁盤の目の移動距離）。\n",
    "    - 高次元や外れ値に強く、頑健な評価をしたい場合に有効。\n",
    "\n",
    " 3. chebyshev（チェビシェフ距離）:\n",
    "    - 各次元の差の中で最大のものを距離とする。\n",
    "    - 一部の次元で大きな違いがあるときに顕著に効く。\n",
    "\n",
    " 4. minkowski（ミンコフスキー距離）:\n",
    "    - ユークリッドやマンハッタンを含む一般化された距離。\n",
    "    - パラメータpにより距離の性質を調整できる（p=1→マンハッタン, p=2→ユークリッド）。\n",
    "\n",
    " 5. hamming（ハミング距離）:\n",
    "    - 同じ長さのベクトルに対して、異なる位置の数をカウント。\n",
    "    - 主にバイナリデータやカテゴリデータの比較に使用。\n",
    "\n",
    " 6. cosine（コサイン距離）:\n",
    "    - ベクトルのなす角に基づいた距離（1 - 類似度）。\n",
    "    - 長さではなく「向き」に注目。高次元で疎なデータ（例：テキスト）に強い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実行コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['euclidean', 'manhattan', 'chebyshev', 'minkowski', 'hamming', 'cosine']:\n",
    "    residual_df = extract_pca_residues(target, 1)\n",
    "    umaped_df = apply_UMAP(residual_df, n_components=2, min_dist = 0, n_neighbors = 5, metric=metric)\n",
    "    df = add_company_name(umaped_df, stock_dfs['list'])\n",
    "    df = apply_xmeans(df, kmax=50, tolerrance=0.01)\n",
    "    print(f'metric: {metric}')\n",
    "    show_in_plotly(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
