{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作モジュール\n",
    "from utils.paths import Paths\n",
    "from acquisition.jquants_api_operations import StockAcquisitionFacade\n",
    "from utils.jquants_api_utils import cli\n",
    "from calculation.target import TargetCalculator\n",
    "# 基本モジュール\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# クラスタリングで使用\n",
    "import umap\n",
    "from pyclustering.cluster.xmeans import xmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 銘柄情報の取得\n",
    "* stock_lists: 2014年10月からの銘柄一覧\n",
    "* history_list: 銘柄ごとのScaleCategoryの遍歴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_codes = [\"4912\",\"4917\",\"7956\",\"8113\",\"4919\",\"4928\",\"6630\",\"7744\",\"4452\",\"4967\",\"8283\",\"4028\",\"4911\",\"4922\",\"4927\"]\n",
    "\n",
    "filter_codes = [str(x) for x in filter_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_condition = \"(Listing==1)&((ScaleCategory=='TOPIX Core30')|(ScaleCategory=='TOPIX Large70')|(ScaleCategory=='TOPIX Mid400')|(ScaleCategory=='TOPIX Small 1'))\" #現行のTOPIX500\"\"\n",
    "saf = StockAcquisitionFacade(filter=filter_condition) #filtered_code_list = filter_codes)\n",
    "stock_dfs = saf.get_stock_data_dict()\n",
    "stock_dfs['list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 価格情報の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目的変数（日内リターン）を算出\n",
    "stock_dfs['price']['Target'] = stock_dfs['price']['Close'] / stock_dfs['price']['Open'] - 1\n",
    "target = stock_dfs['price'][['Date', 'Code', 'Target']]\n",
    "target = target.set_index(['Date', 'Code'], drop=True).unstack(-1).droplevel(0, axis=1)\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA処理の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "end_date = datetime(2022,1,1)\n",
    "n_components = 115 # 累積寄与率がおおむね80%となる要素数\n",
    "\n",
    "\n",
    "target = target[target.index <= end_date]\n",
    "no_missing_residuals = target.dropna(axis=1).T\n",
    "\n",
    "pca = PCA(n_components = n_components).fit(no_missing_residuals)\n",
    "\n",
    "explained_ratio_df = pd.DataFrame(np.cumsum(pca.explained_variance_ratio_), \n",
    "                                  index=['PC_'+ '{:0=3}'.format(j) for j in range(0, n_components)],\n",
    "                                  columns=['ExplainedRatio'])\n",
    "explained_ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_array = pca.transform(no_missing_residuals)\n",
    "\n",
    "pca_df = pd.DataFrame(pca_array, index=no_missing_residuals.index, columns=['PC_'+ '{:0=3}'.format(j) for j in range(0, n_components)])\n",
    "pca_df = pca_df[[x for x in pca_df.columns if x not in ['PC_000', 'PC_002', 'PC_003', 'PC_004']]]\n",
    "extracted_df = pca_df.sort_index(ascending=True)\n",
    "extracted_df.to_csv('pca_residue.csv')\n",
    "extracted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリングの関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAPを適用する関数\n",
    "def apply_UMAP(df:pd.DataFrame, n_components:int=15, n_neighbors:int=5, min_dist:float=0.1):\n",
    "    UMAP_model = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist)\n",
    "    UMAP_result = UMAP_model.fit_transform(df)\n",
    "    reduced_df = pd.DataFrame(UMAP_result, index=df.index, columns=['Feature '+ str(i) for i in range(0, n_components)])\n",
    "    reduced_df = reduced_df.sort_index(ascending=True)\n",
    "    return reduced_df\n",
    "\n",
    "# データフレームにxmeansを適用\n",
    "def apply_xmeans(df:pd.DataFrame, n_iteration:int=30, kmax=10):\n",
    "    dfs_xmeans = []\n",
    "    for i in range(0, n_iteration):\n",
    "        df_xmeans_cluster_center = _calculate_xmeans_cluster_center(df, kmax=kmax)\n",
    "        dfs_xmeans.append(df_xmeans_cluster_center)\n",
    "    dfs_xmeans = pd.concat(dfs_xmeans, axis=0)\n",
    "    dfs_xmeans = dfs_xmeans.reset_index().drop('index', axis=1)\n",
    "    df = dfs_xmeans.groupby('Code').mean()\n",
    "    return df\n",
    "\n",
    "# xmeansのクラスタ中心を算出するサブ関数\n",
    "def _calculate_xmeans_cluster_center(df, kmax):\n",
    "    xm_c = kmeans_plusplus_initializer(data=df, amount_centers=2).initialize()\n",
    "    xm_i = xmeans(data=df, initial_centers=xm_c, kmax=kmax, tolerrance=0.001, ccore=True)\n",
    "    xm_i.process()\n",
    "\n",
    "    for cluster_num, cluster in enumerate(xm_i._xmeans__clusters):\n",
    "        for index_num in cluster:\n",
    "            df.loc[df.index[index_num]] = np.array(xm_i._xmeans__centers, dtype=np.float32)[cluster_num, :]\n",
    "\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "# ユークリッド距離からクラスタを決定する関数\n",
    "def determine_cluster_from_euclidean(df:pd.DataFrame, theshold_column='Distance', cluster_theshold=0.03):\n",
    "    df_euclidean = _calculate_euclidean(df=df)\n",
    "    df_clustered = _determine_cluster(df=df_euclidean, theshold_column=theshold_column, cluster_theshold=cluster_theshold)\n",
    "    return df_clustered\n",
    "\n",
    "# 銘柄間のユークリッド距離を算出するサブ関数\n",
    "def _calculate_euclidean(df:pd.DataFrame):\n",
    "    np_euclidean = cdist(df.values, df.values, metric='euclidean')\n",
    "    df_euclidean = pd.DataFrame(np_euclidean, index=df.index, columns=df.index)\n",
    "    return df_euclidean\n",
    "\n",
    "# ユークリッド距離からクラスタを決定するサブ関数\n",
    "def _determine_cluster(df:pd.DataFrame, theshold_column:str, cluster_theshold:float):\n",
    "    df_unstacked = pd.DataFrame(df.unstack(), columns=[theshold_column])\n",
    "    in_theshold = df_unstacked[theshold_column]<=cluster_theshold\n",
    "    df_in_theshold = df_unstacked[in_theshold].sort_values(theshold_column, ascending=True)\n",
    "    df_in_theshold['Cluster'] = np.nan\n",
    "\n",
    "    cluster_num = 0\n",
    "    for i in df_in_theshold.index.get_level_values(0).unique():\n",
    "\n",
    "        clustering_completed = df_in_theshold['Cluster'].isnull().any() == False\n",
    "        if clustering_completed:\n",
    "            break\n",
    "        cluster_is_undetermined = \\\n",
    "          df_in_theshold.loc[df_in_theshold.index.get_level_values(0)==i, 'Cluster'].isnull().all()\n",
    "        if cluster_is_undetermined:\n",
    "            df_in_theshold.loc[df_in_theshold.index.get_level_values(0)==i, 'Cluster'] = cluster_num\n",
    "            df_in_theshold.loc[df_in_theshold.index.get_level_values(1)==i, 'Cluster'] = cluster_num\n",
    "            cluster_num += 1\n",
    "        else:\n",
    "            df_in_theshold.loc[df_in_theshold.index.get_level_values(0)==i, 'Cluster'] = \\\n",
    "                int(df_in_theshold.loc[df_in_theshold.index.get_level_values(0)==i, 'Cluster'].mean())\n",
    "            df_in_theshold.loc[df_in_theshold.index.get_level_values(1)==i, 'Cluster'] = \\\n",
    "                int(df_in_theshold.loc[df_in_theshold.index.get_level_values(0)==i, 'Cluster'].mean())\n",
    "\n",
    "    df_in_theshold = df_in_theshold[['Cluster']].astype(int).reset_index(level=0, drop=True)\n",
    "    df_in_theshold = df_in_theshold.loc[~df_in_theshold.index.duplicated(keep='first')]\n",
    "\n",
    "    df_clustered = pd.merge(\n",
    "                            df_in_theshold, stock_dfs['list'],\n",
    "                            how='left', on='Code',\n",
    "                            ).sort_values(by=['Cluster', 'Code'], ascending=True).set_index('Code', drop=True)\n",
    "    return df_clustered[['CompanyName', 'MarketCodeName', 'Sector33CodeName', 'Sector17CodeName', 'ScaleCategory', 'Listing', 'Cluster']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラスタリング(1段階目)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_codes = [\"7164\",\"8473\",\"8698\",\"8604\",\"8609\",\"8613\",\"8616\",\"8628\",\"8706\",\"8601\",\"8511\",\"8697\",\"8130\",\"7981\",\"5929\",\"5943\",\"9934\",\"5930\",\"7943\",\"6651\",\"7148\",\"5938\",\"1808\",\"8897\",\"1419\",\"1928\",\"3291\",\"4204\",\"8848\",\"1911\",\"1878\",\"1925\",\"1766\",\"8923\",\"8905\",\"8803\",\"3289\",\"8801\",\"8802\",\"8804\",\"8830\",\"3231\",\"3003\",\"8934\",\"8871\"]\n",
    "\n",
    "extracted_df = extracted_df.loc[filter_codes, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_dfs = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    umaped_df = apply_UMAP(extracted_df)\n",
    "    df_after_xmeans = apply_xmeans(umaped_df, kmax=50)\n",
    "    clustered_df = determine_cluster_from_euclidean(df_after_xmeans, cluster_theshold=0.03)\n",
    "    if len(clustered_dfs) != 0:\n",
    "        clustered_df = clustered_df[['Cluster']]\n",
    "    clustered_df = clustered_df.rename(columns={'Cluster': f'Cluster_{i}'})\n",
    "    clustered_dfs = pd.merge(clustered_dfs, clustered_df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "clustered_dfs.to_csv('cluster1000_2.csv')\n",
    "clustered_dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
